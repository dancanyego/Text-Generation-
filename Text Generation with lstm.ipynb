{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd941f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    \n",
    "    with open (filepath) as f:\n",
    "        str_text = f.read()\n",
    "    \n",
    "    return str_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac7971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d22e54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a640ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg',disable=['parser', 'tagger','ner']) # Disabling so as it processes much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193ea6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d36aaffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1528f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "156e232d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\dataenv\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200bff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11338"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02d045a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will pass 25 words and let neural net predict 26th word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f153a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 25 + 1\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len,len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7efca08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d5cbba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6af41c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5c39270",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ec52588",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfca1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57436257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "956 :> call\n",
      "14 :> me\n",
      "263 :> ishmael\n",
      "51 :> some\n",
      "261 :> years\n",
      "408 :> ago\n",
      "87 :> never\n",
      "219 :> mind\n",
      "129 :> how\n",
      "111 :> long\n",
      "954 :> precisely\n",
      "260 :> having\n",
      "50 :> little\n",
      "43 :> or\n",
      "38 :> no\n",
      "314 :> money\n",
      "7 :> in\n",
      "23 :> my\n",
      "546 :> purse\n",
      "3 :> and\n",
      "150 :> nothing\n",
      "259 :> particular\n",
      "6 :> to\n",
      "2713 :> interest\n",
      "14 :> me\n",
      "24 :> on\n"
     ]
    }
   ],
   "source": [
    "for i in sequences[0]:\n",
    "    print(f\"{i} :> {tokenizer.index_word[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53f1dcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2718"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1acd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b99255aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "603c8d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 956,   14,  263, ..., 2713,   14,   24],\n",
       "       [  14,  263,   51, ...,   14,   24,  957],\n",
       "       [ 263,   51,  261, ...,   24,  957,    5],\n",
       "       ...,\n",
       "       [ 952,   12,  166, ...,  262,   53,    2],\n",
       "       [  12,  166, 2712, ...,   53,    2, 2718],\n",
       "       [ 166, 2712,    3, ...,    2, 2718,   26]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cb65a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM based model and train test split\n",
    "vocabulary_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b6ee3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cee9c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "810c689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y,num_classes=vocabulary_size + 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8493e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "898fc80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11312, 25)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8607afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding    # Lstm to handle sequences , Embedding layer to handle Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6143a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b0deb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 25)            67975     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 25, 150)           105600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 150)               180600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2719)              410569    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 787,394\n",
      "Trainable params: 787,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "edb8e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df09bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/166\n",
      "89/89 [==============================] - 23s 194ms/step - loss: 6.8626 - accuracy: 0.0511\n",
      "Epoch 2/166\n",
      "89/89 [==============================] - 18s 198ms/step - loss: 6.3897 - accuracy: 0.0529\n",
      "Epoch 3/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 6.3474 - accuracy: 0.0529\n",
      "Epoch 4/166\n",
      "89/89 [==============================] - 18s 199ms/step - loss: 6.2196 - accuracy: 0.0529\n",
      "Epoch 5/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 6.1249 - accuracy: 0.0530\n",
      "Epoch 6/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 6.0228 - accuracy: 0.0594\n",
      "Epoch 7/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 5.8867 - accuracy: 0.0667\n",
      "Epoch 8/166\n",
      "89/89 [==============================] - 17s 196ms/step - loss: 5.7920 - accuracy: 0.0696\n",
      "Epoch 9/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 5.7084 - accuracy: 0.0719\n",
      "Epoch 10/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 5.6359 - accuracy: 0.0744\n",
      "Epoch 11/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 5.5685 - accuracy: 0.0771\n",
      "Epoch 12/166\n",
      "89/89 [==============================] - 17s 193ms/step - loss: 5.5064 - accuracy: 0.0765\n",
      "Epoch 13/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 5.4466 - accuracy: 0.0812\n",
      "Epoch 14/166\n",
      "89/89 [==============================] - 18s 199ms/step - loss: 5.3860 - accuracy: 0.0840\n",
      "Epoch 15/166\n",
      "89/89 [==============================] - 18s 198ms/step - loss: 5.3277 - accuracy: 0.0841\n",
      "Epoch 16/166\n",
      "89/89 [==============================] - 18s 198ms/step - loss: 5.2746 - accuracy: 0.0881\n",
      "Epoch 17/166\n",
      "89/89 [==============================] - 17s 196ms/step - loss: 5.2218 - accuracy: 0.0850\n",
      "Epoch 18/166\n",
      "89/89 [==============================] - 17s 196ms/step - loss: 5.1710 - accuracy: 0.0883\n",
      "Epoch 19/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 5.1186 - accuracy: 0.0896\n",
      "Epoch 20/166\n",
      "89/89 [==============================] - 17s 193ms/step - loss: 5.0709 - accuracy: 0.0931\n",
      "Epoch 21/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 5.0214 - accuracy: 0.0919\n",
      "Epoch 22/166\n",
      "89/89 [==============================] - 17s 193ms/step - loss: 4.9709 - accuracy: 0.0925\n",
      "Epoch 23/166\n",
      "89/89 [==============================] - 17s 193ms/step - loss: 4.9314 - accuracy: 0.0947\n",
      "Epoch 24/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 4.8803 - accuracy: 0.0960\n",
      "Epoch 25/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 4.8346 - accuracy: 0.0967\n",
      "Epoch 26/166\n",
      "89/89 [==============================] - 18s 198ms/step - loss: 4.7885 - accuracy: 0.0993\n",
      "Epoch 27/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 4.7566 - accuracy: 0.0989\n",
      "Epoch 28/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 4.6994 - accuracy: 0.1023\n",
      "Epoch 29/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 4.6511 - accuracy: 0.1028\n",
      "Epoch 30/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 4.6060 - accuracy: 0.1040\n",
      "Epoch 31/166\n",
      "89/89 [==============================] - 17s 196ms/step - loss: 4.5621 - accuracy: 0.1078\n",
      "Epoch 32/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 4.5186 - accuracy: 0.1077\n",
      "Epoch 33/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 4.4792 - accuracy: 0.1078\n",
      "Epoch 34/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 4.4433 - accuracy: 0.1143\n",
      "Epoch 35/166\n",
      "89/89 [==============================] - 18s 197ms/step - loss: 4.3964 - accuracy: 0.1160\n",
      "Epoch 36/166\n",
      "89/89 [==============================] - 18s 195ms/step - loss: 4.3517 - accuracy: 0.1152\n",
      "Epoch 37/166\n",
      "89/89 [==============================] - 17s 196ms/step - loss: 4.3120 - accuracy: 0.1175\n",
      "Epoch 38/166\n",
      "89/89 [==============================] - 18s 198ms/step - loss: 4.2774 - accuracy: 0.1202\n",
      "Epoch 39/166\n",
      "89/89 [==============================] - 18s 197ms/step - loss: 4.2374 - accuracy: 0.1205\n",
      "Epoch 40/166\n",
      "89/89 [==============================] - 18s 197ms/step - loss: 4.1936 - accuracy: 0.1238\n",
      "Epoch 41/166\n",
      "89/89 [==============================] - 18s 198ms/step - loss: 4.1562 - accuracy: 0.1246\n",
      "Epoch 42/166\n",
      "89/89 [==============================] - 18s 200ms/step - loss: 4.1129 - accuracy: 0.1302\n",
      "Epoch 43/166\n",
      "89/89 [==============================] - 18s 200ms/step - loss: 4.0734 - accuracy: 0.1315\n",
      "Epoch 44/166\n",
      "89/89 [==============================] - 18s 197ms/step - loss: 4.0403 - accuracy: 0.1335\n",
      "Epoch 45/166\n",
      "89/89 [==============================] - 16s 183ms/step - loss: 4.0007 - accuracy: 0.1353\n",
      "Epoch 46/166\n",
      "89/89 [==============================] - 16s 182ms/step - loss: 3.9654 - accuracy: 0.1432\n",
      "Epoch 47/166\n",
      "89/89 [==============================] - 16s 184ms/step - loss: 3.9320 - accuracy: 0.1445\n",
      "Epoch 48/166\n",
      "89/89 [==============================] - 16s 184ms/step - loss: 3.9058 - accuracy: 0.1472\n",
      "Epoch 49/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 3.8668 - accuracy: 0.1482\n",
      "Epoch 50/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 3.8265 - accuracy: 0.1560\n",
      "Epoch 51/166\n",
      "89/89 [==============================] - 16s 181ms/step - loss: 3.7888 - accuracy: 0.1581\n",
      "Epoch 52/166\n",
      "89/89 [==============================] - 16s 183ms/step - loss: 3.7602 - accuracy: 0.1608\n",
      "Epoch 53/166\n",
      "89/89 [==============================] - 16s 183ms/step - loss: 3.7269 - accuracy: 0.1637\n",
      "Epoch 54/166\n",
      "89/89 [==============================] - 16s 181ms/step - loss: 3.7004 - accuracy: 0.1676\n",
      "Epoch 55/166\n",
      "89/89 [==============================] - 16s 181ms/step - loss: 3.6592 - accuracy: 0.1734\n",
      "Epoch 56/166\n",
      "89/89 [==============================] - 16s 183ms/step - loss: 3.6400 - accuracy: 0.1735\n",
      "Epoch 57/166\n",
      "89/89 [==============================] - 20s 221ms/step - loss: 3.6046 - accuracy: 0.1800\n",
      "Epoch 58/166\n",
      "89/89 [==============================] - 22s 246ms/step - loss: 3.5770 - accuracy: 0.1796\n",
      "Epoch 59/166\n",
      "89/89 [==============================] - 26s 289ms/step - loss: 3.5452 - accuracy: 0.1844\n",
      "Epoch 60/166\n",
      "89/89 [==============================] - 23s 255ms/step - loss: 3.5167 - accuracy: 0.1904\n",
      "Epoch 61/166\n",
      "89/89 [==============================] - 23s 256ms/step - loss: 3.4890 - accuracy: 0.1925\n",
      "Epoch 62/166\n",
      "89/89 [==============================] - 22s 252ms/step - loss: 3.4580 - accuracy: 0.1947\n",
      "Epoch 63/166\n",
      "89/89 [==============================] - 22s 249ms/step - loss: 3.4261 - accuracy: 0.2026\n",
      "Epoch 64/166\n",
      "89/89 [==============================] - 20s 224ms/step - loss: 3.3993 - accuracy: 0.2020\n",
      "Epoch 65/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 3.3705 - accuracy: 0.2121\n",
      "Epoch 66/166\n",
      "89/89 [==============================] - 16s 185ms/step - loss: 3.3450 - accuracy: 0.2125\n",
      "Epoch 67/166\n",
      "89/89 [==============================] - 16s 181ms/step - loss: 3.3178 - accuracy: 0.2155\n",
      "Epoch 68/166\n",
      "89/89 [==============================] - 25s 280ms/step - loss: 3.2877 - accuracy: 0.2216\n",
      "Epoch 69/166\n",
      "89/89 [==============================] - 18s 208ms/step - loss: 3.2688 - accuracy: 0.2245\n",
      "Epoch 70/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 3.2373 - accuracy: 0.2303\n",
      "Epoch 71/166\n",
      "89/89 [==============================] - 18s 202ms/step - loss: 3.2152 - accuracy: 0.2340\n",
      "Epoch 72/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 3.1928 - accuracy: 0.2376\n",
      "Epoch 73/166\n",
      "89/89 [==============================] - 19s 215ms/step - loss: 3.1610 - accuracy: 0.2446\n",
      "Epoch 74/166\n",
      "89/89 [==============================] - 20s 224ms/step - loss: 3.1387 - accuracy: 0.2456\n",
      "Epoch 75/166\n",
      "89/89 [==============================] - 21s 235ms/step - loss: 3.1104 - accuracy: 0.2511\n",
      "Epoch 76/166\n",
      "89/89 [==============================] - 18s 200ms/step - loss: 3.0814 - accuracy: 0.2577\n",
      "Epoch 77/166\n",
      "89/89 [==============================] - 16s 185ms/step - loss: 3.0608 - accuracy: 0.2615\n",
      "Epoch 78/166\n",
      "89/89 [==============================] - 17s 189ms/step - loss: 3.0364 - accuracy: 0.2669\n",
      "Epoch 79/166\n",
      "89/89 [==============================] - 19s 209ms/step - loss: 3.0108 - accuracy: 0.2710\n",
      "Epoch 80/166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 18s 208ms/step - loss: 2.9918 - accuracy: 0.2699\n",
      "Epoch 81/166\n",
      "89/89 [==============================] - 19s 213ms/step - loss: 2.9799 - accuracy: 0.2764\n",
      "Epoch 82/166\n",
      "89/89 [==============================] - 23s 263ms/step - loss: 2.9507 - accuracy: 0.2831\n",
      "Epoch 83/166\n",
      "89/89 [==============================] - 19s 209ms/step - loss: 2.9220 - accuracy: 0.2862\n",
      "Epoch 84/166\n",
      "89/89 [==============================] - 19s 214ms/step - loss: 2.8881 - accuracy: 0.2976\n",
      "Epoch 85/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.8651 - accuracy: 0.2988\n",
      "Epoch 86/166\n",
      "89/89 [==============================] - 21s 233ms/step - loss: 2.8484 - accuracy: 0.3036\n",
      "Epoch 87/166\n",
      "89/89 [==============================] - 18s 207ms/step - loss: 2.8284 - accuracy: 0.3092\n",
      "Epoch 88/166\n",
      "89/89 [==============================] - 20s 227ms/step - loss: 2.7984 - accuracy: 0.3159\n",
      "Epoch 89/166\n",
      "89/89 [==============================] - 20s 226ms/step - loss: 2.7798 - accuracy: 0.3141\n",
      "Epoch 90/166\n",
      "89/89 [==============================] - 26s 290ms/step - loss: 2.7556 - accuracy: 0.3237\n",
      "Epoch 91/166\n",
      "89/89 [==============================] - 20s 227ms/step - loss: 2.7410 - accuracy: 0.3218\n",
      "Epoch 92/166\n",
      "89/89 [==============================] - 18s 205ms/step - loss: 2.7274 - accuracy: 0.3246\n",
      "Epoch 93/166\n",
      "89/89 [==============================] - 20s 227ms/step - loss: 2.7072 - accuracy: 0.3292\n",
      "Epoch 94/166\n",
      "89/89 [==============================] - 21s 238ms/step - loss: 2.6826 - accuracy: 0.3369\n",
      "Epoch 95/166\n",
      "89/89 [==============================] - 19s 212ms/step - loss: 2.6644 - accuracy: 0.3400\n",
      "Epoch 96/166\n",
      "89/89 [==============================] - 18s 199ms/step - loss: 2.6417 - accuracy: 0.3491\n",
      "Epoch 97/166\n",
      "89/89 [==============================] - 17s 189ms/step - loss: 2.6236 - accuracy: 0.3518\n",
      "Epoch 98/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 2.6044 - accuracy: 0.3490\n",
      "Epoch 99/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 2.5762 - accuracy: 0.3620\n",
      "Epoch 100/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.5533 - accuracy: 0.3655\n",
      "Epoch 101/166\n",
      "89/89 [==============================] - 18s 197ms/step - loss: 2.5357 - accuracy: 0.3684\n",
      "Epoch 102/166\n",
      "89/89 [==============================] - 18s 200ms/step - loss: 2.5294 - accuracy: 0.3700\n",
      "Epoch 103/166\n",
      "89/89 [==============================] - 18s 197ms/step - loss: 2.5071 - accuracy: 0.3755\n",
      "Epoch 104/166\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 2.4986 - accuracy: 0.3782\n",
      "Epoch 105/166\n",
      "89/89 [==============================] - 18s 199ms/step - loss: 2.4721 - accuracy: 0.3810\n",
      "Epoch 106/166\n",
      "89/89 [==============================] - 16s 176ms/step - loss: 2.4591 - accuracy: 0.3834\n",
      "Epoch 107/166\n",
      "89/89 [==============================] - 25s 283ms/step - loss: 2.4430 - accuracy: 0.3899\n",
      "Epoch 108/166\n",
      "89/89 [==============================] - 26s 288ms/step - loss: 2.4260 - accuracy: 0.3906\n",
      "Epoch 109/166\n",
      "89/89 [==============================] - 23s 262ms/step - loss: 2.4092 - accuracy: 0.3944\n",
      "Epoch 110/166\n",
      "89/89 [==============================] - 28s 316ms/step - loss: 2.3937 - accuracy: 0.3986\n",
      "Epoch 111/166\n",
      "89/89 [==============================] - 27s 299ms/step - loss: 2.3686 - accuracy: 0.4073\n",
      "Epoch 112/166\n",
      "89/89 [==============================] - 23s 259ms/step - loss: 2.3541 - accuracy: 0.4085\n",
      "Epoch 113/166\n",
      "89/89 [==============================] - 24s 266ms/step - loss: 2.3357 - accuracy: 0.4140\n",
      "Epoch 114/166\n",
      "89/89 [==============================] - 26s 293ms/step - loss: 2.3215 - accuracy: 0.4104\n",
      "Epoch 115/166\n",
      "89/89 [==============================] - 23s 260ms/step - loss: 2.3059 - accuracy: 0.4188\n",
      "Epoch 116/166\n",
      "89/89 [==============================] - 23s 254ms/step - loss: 2.3048 - accuracy: 0.4179\n",
      "Epoch 117/166\n",
      "89/89 [==============================] - 23s 255ms/step - loss: 2.2820 - accuracy: 0.4244\n",
      "Epoch 118/166\n",
      "89/89 [==============================] - 23s 254ms/step - loss: 2.2643 - accuracy: 0.4280\n",
      "Epoch 119/166\n",
      "89/89 [==============================] - 23s 257ms/step - loss: 2.2548 - accuracy: 0.4303\n",
      "Epoch 120/166\n",
      "89/89 [==============================] - 23s 254ms/step - loss: 2.2411 - accuracy: 0.4328\n",
      "Epoch 121/166\n",
      "89/89 [==============================] - 23s 258ms/step - loss: 2.2197 - accuracy: 0.4427\n",
      "Epoch 122/166\n",
      "62/89 [===================>..........] - ETA: 7s - loss: 2.1506 - accuracy: 0.4534"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=166,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaaad1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
