{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd941f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    \n",
    "    with open (filepath) as f:\n",
    "        str_text = f.read()\n",
    "    \n",
    "    return str_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac7971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d22e54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a640ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg',disable=['parser', 'tagger','ner']) # Disabling so as it processes much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "193ea6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d36aaffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1528f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('moby_dick_four_chapters.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "156e232d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\dataenv\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "200bff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11338"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02d045a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will pass 25 words and let neural net predict 26th word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f153a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 25 + 1\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len,len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7efca08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d5cbba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on shore'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(text_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6af41c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5c39270",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ec52588",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfca1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57436257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "956 :> call\n",
      "14 :> me\n",
      "263 :> ishmael\n",
      "51 :> some\n",
      "261 :> years\n",
      "408 :> ago\n",
      "87 :> never\n",
      "219 :> mind\n",
      "129 :> how\n",
      "111 :> long\n",
      "954 :> precisely\n",
      "260 :> having\n",
      "50 :> little\n",
      "43 :> or\n",
      "38 :> no\n",
      "314 :> money\n",
      "7 :> in\n",
      "23 :> my\n",
      "546 :> purse\n",
      "3 :> and\n",
      "150 :> nothing\n",
      "259 :> particular\n",
      "6 :> to\n",
      "2713 :> interest\n",
      "14 :> me\n",
      "24 :> on\n"
     ]
    }
   ],
   "source": [
    "for i in sequences[0]:\n",
    "    print(f\"{i} :> {tokenizer.index_word[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53f1dcf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2718"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1acd829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b99255aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "603c8d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 956,   14,  263, ..., 2713,   14,   24],\n",
       "       [  14,  263,   51, ...,   14,   24,  957],\n",
       "       [ 263,   51,  261, ...,   24,  957,    5],\n",
       "       ...,\n",
       "       [ 952,   12,  166, ...,  262,   53,    2],\n",
       "       [  12,  166, 2712, ...,   53,    2, 2718],\n",
       "       [ 166, 2712,    3, ...,    2, 2718,   26]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cb65a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM based model and train test split\n",
    "vocabulary_size = len(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b6ee3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cee9c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "810c689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y,num_classes=vocabulary_size + 1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8493e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "898fc80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11312, 25)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8607afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding    # Lstm to handle sequences , Embedding layer to handle Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6143a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b0deb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 25)            67975     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 25, 150)           105600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 150)               180600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2719)              410569    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 787,394\n",
      "Trainable params: 787,394\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "edb8e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump,load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0df09bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/166\n",
      "89/89 [==============================] - 27s 223ms/step - loss: 6.8659 - accuracy: 0.0444\n",
      "Epoch 2/166\n",
      "89/89 [==============================] - 22s 253ms/step - loss: 6.3885 - accuracy: 0.0529\n",
      "Epoch 3/166\n",
      "89/89 [==============================] - 21s 239ms/step - loss: 6.3532 - accuracy: 0.0529\n",
      "Epoch 4/166\n",
      "89/89 [==============================] - 18s 207ms/step - loss: 6.2467 - accuracy: 0.0529\n",
      "Epoch 5/166\n",
      "89/89 [==============================] - 23s 262ms/step - loss: 6.1262 - accuracy: 0.0529\n",
      "Epoch 6/166\n",
      "89/89 [==============================] - 25s 281ms/step - loss: 5.9946 - accuracy: 0.0602\n",
      "Epoch 7/166\n",
      "89/89 [==============================] - 20s 228ms/step - loss: 5.8841 - accuracy: 0.0667\n",
      "Epoch 8/166\n",
      "89/89 [==============================] - 19s 215ms/step - loss: 5.8113 - accuracy: 0.0660\n",
      "Epoch 9/166\n",
      "89/89 [==============================] - 20s 219ms/step - loss: 5.7291 - accuracy: 0.0684\n",
      "Epoch 10/166\n",
      "89/89 [==============================] - 23s 253ms/step - loss: 5.6505 - accuracy: 0.0728\n",
      "Epoch 11/166\n",
      "89/89 [==============================] - 27s 309ms/step - loss: 5.5816 - accuracy: 0.0750\n",
      "Epoch 12/166\n",
      "89/89 [==============================] - 22s 253ms/step - loss: 5.5243 - accuracy: 0.0788\n",
      "Epoch 13/166\n",
      "89/89 [==============================] - 23s 259ms/step - loss: 5.4691 - accuracy: 0.0803\n",
      "Epoch 14/166\n",
      "89/89 [==============================] - 24s 274ms/step - loss: 5.4154 - accuracy: 0.0805\n",
      "Epoch 15/166\n",
      "89/89 [==============================] - 19s 218ms/step - loss: 5.3609 - accuracy: 0.0841\n",
      "Epoch 16/166\n",
      "89/89 [==============================] - 24s 274ms/step - loss: 5.3128 - accuracy: 0.0862\n",
      "Epoch 17/166\n",
      "89/89 [==============================] - 23s 264ms/step - loss: 5.2594 - accuracy: 0.0892\n",
      "Epoch 18/166\n",
      "89/89 [==============================] - 24s 272ms/step - loss: 5.2123 - accuracy: 0.0901\n",
      "Epoch 19/166\n",
      "89/89 [==============================] - 22s 248ms/step - loss: 5.1724 - accuracy: 0.0883\n",
      "Epoch 20/166\n",
      "89/89 [==============================] - 23s 259ms/step - loss: 5.1276 - accuracy: 0.0939\n",
      "Epoch 21/166\n",
      "89/89 [==============================] - 22s 247ms/step - loss: 5.0853 - accuracy: 0.0928\n",
      "Epoch 22/166\n",
      "89/89 [==============================] - 21s 237ms/step - loss: 5.0448 - accuracy: 0.0951\n",
      "Epoch 23/166\n",
      "89/89 [==============================] - 21s 235ms/step - loss: 5.0005 - accuracy: 0.0930\n",
      "Epoch 24/166\n",
      "89/89 [==============================] - 19s 208ms/step - loss: 4.9610 - accuracy: 0.0948\n",
      "Epoch 25/166\n",
      "89/89 [==============================] - 18s 202ms/step - loss: 4.9213 - accuracy: 0.0936\n",
      "Epoch 26/166\n",
      "89/89 [==============================] - 18s 207ms/step - loss: 4.8918 - accuracy: 0.0949\n",
      "Epoch 27/166\n",
      "89/89 [==============================] - 18s 200ms/step - loss: 4.8472 - accuracy: 0.0967\n",
      "Epoch 28/166\n",
      "89/89 [==============================] - 20s 231ms/step - loss: 4.8010 - accuracy: 0.0988\n",
      "Epoch 29/166\n",
      "89/89 [==============================] - 27s 303ms/step - loss: 4.7626 - accuracy: 0.0989\n",
      "Epoch 30/166\n",
      "89/89 [==============================] - 30s 334ms/step - loss: 4.7297 - accuracy: 0.0986\n",
      "Epoch 31/166\n",
      "89/89 [==============================] - 25s 283ms/step - loss: 4.6896 - accuracy: 0.1016\n",
      "Epoch 32/166\n",
      "89/89 [==============================] - 19s 214ms/step - loss: 4.6555 - accuracy: 0.1002\n",
      "Epoch 33/166\n",
      "89/89 [==============================] - 21s 239ms/step - loss: 4.6222 - accuracy: 0.1018\n",
      "Epoch 34/166\n",
      "89/89 [==============================] - 24s 261ms/step - loss: 4.5900 - accuracy: 0.1014\n",
      "Epoch 35/166\n",
      "89/89 [==============================] - 23s 259ms/step - loss: 4.5423 - accuracy: 0.1033\n",
      "Epoch 36/166\n",
      "89/89 [==============================] - 20s 228ms/step - loss: 4.5024 - accuracy: 0.1048\n",
      "Epoch 37/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 4.4652 - accuracy: 0.1067\n",
      "Epoch 38/166\n",
      "89/89 [==============================] - 22s 243ms/step - loss: 4.4255 - accuracy: 0.1072\n",
      "Epoch 39/166\n",
      "89/89 [==============================] - 19s 208ms/step - loss: 4.3857 - accuracy: 0.1151\n",
      "Epoch 40/166\n",
      "89/89 [==============================] - 20s 221ms/step - loss: 4.3491 - accuracy: 0.1135\n",
      "Epoch 41/166\n",
      "89/89 [==============================] - 26s 289ms/step - loss: 4.3120 - accuracy: 0.1167\n",
      "Epoch 42/166\n",
      "89/89 [==============================] - 19s 215ms/step - loss: 4.2742 - accuracy: 0.1189\n",
      "Epoch 43/166\n",
      "89/89 [==============================] - 18s 202ms/step - loss: 4.2334 - accuracy: 0.1212\n",
      "Epoch 44/166\n",
      "89/89 [==============================] - 21s 234ms/step - loss: 4.1944 - accuracy: 0.1239\n",
      "Epoch 45/166\n",
      "89/89 [==============================] - 22s 246ms/step - loss: 4.1621 - accuracy: 0.1227\n",
      "Epoch 46/166\n",
      "89/89 [==============================] - 26s 290ms/step - loss: 4.1226 - accuracy: 0.1299\n",
      "Epoch 47/166\n",
      "89/89 [==============================] - 21s 240ms/step - loss: 4.0933 - accuracy: 0.1298\n",
      "Epoch 48/166\n",
      "89/89 [==============================] - 19s 211ms/step - loss: 4.0557 - accuracy: 0.1329\n",
      "Epoch 49/166\n",
      "89/89 [==============================] - 19s 213ms/step - loss: 4.0197 - accuracy: 0.1362\n",
      "Epoch 50/166\n",
      "89/89 [==============================] - 19s 213ms/step - loss: 3.9780 - accuracy: 0.1386\n",
      "Epoch 51/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 3.9434 - accuracy: 0.1427\n",
      "Epoch 52/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 3.9083 - accuracy: 0.1443\n",
      "Epoch 53/166\n",
      "89/89 [==============================] - 21s 241ms/step - loss: 3.8711 - accuracy: 0.1498\n",
      "Epoch 54/166\n",
      "89/89 [==============================] - 22s 241ms/step - loss: 3.8395 - accuracy: 0.1537\n",
      "Epoch 55/166\n",
      "89/89 [==============================] - 21s 242ms/step - loss: 3.8032 - accuracy: 0.1558\n",
      "Epoch 56/166\n",
      "89/89 [==============================] - 22s 248ms/step - loss: 3.7717 - accuracy: 0.1586\n",
      "Epoch 57/166\n",
      "89/89 [==============================] - 29s 319ms/step - loss: 3.7335 - accuracy: 0.1613\n",
      "Epoch 58/166\n",
      "89/89 [==============================] - 23s 262ms/step - loss: 3.7027 - accuracy: 0.1679\n",
      "Epoch 59/166\n",
      "89/89 [==============================] - 24s 266ms/step - loss: 3.6713 - accuracy: 0.1703\n",
      "Epoch 60/166\n",
      "89/89 [==============================] - 27s 297ms/step - loss: 3.6352 - accuracy: 0.1775\n",
      "Epoch 61/166\n",
      "89/89 [==============================] - 24s 273ms/step - loss: 3.6055 - accuracy: 0.1810\n",
      "Epoch 62/166\n",
      "89/89 [==============================] - 24s 270ms/step - loss: 3.5775 - accuracy: 0.1852\n",
      "Epoch 63/166\n",
      "89/89 [==============================] - 25s 281ms/step - loss: 3.5404 - accuracy: 0.1894\n",
      "Epoch 64/166\n",
      "89/89 [==============================] - 18s 197ms/step - loss: 3.5088 - accuracy: 0.1914\n",
      "Epoch 65/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 3.4845 - accuracy: 0.1933\n",
      "Epoch 66/166\n",
      "89/89 [==============================] - 19s 210ms/step - loss: 3.4542 - accuracy: 0.2016\n",
      "Epoch 67/166\n",
      "89/89 [==============================] - 22s 251ms/step - loss: 3.4233 - accuracy: 0.2042\n",
      "Epoch 68/166\n",
      "89/89 [==============================] - 22s 252ms/step - loss: 3.3931 - accuracy: 0.2137\n",
      "Epoch 69/166\n",
      "89/89 [==============================] - 23s 253ms/step - loss: 3.3635 - accuracy: 0.2197\n",
      "Epoch 70/166\n",
      "89/89 [==============================] - 23s 255ms/step - loss: 3.3446 - accuracy: 0.2203\n",
      "Epoch 71/166\n",
      "89/89 [==============================] - 22s 252ms/step - loss: 3.3147 - accuracy: 0.2260\n",
      "Epoch 72/166\n",
      "89/89 [==============================] - 25s 282ms/step - loss: 3.2838 - accuracy: 0.2301\n",
      "Epoch 73/166\n",
      "89/89 [==============================] - 25s 276ms/step - loss: 3.2507 - accuracy: 0.2371\n",
      "Epoch 74/166\n",
      "89/89 [==============================] - 24s 265ms/step - loss: 3.2271 - accuracy: 0.2397\n",
      "Epoch 75/166\n",
      "89/89 [==============================] - 23s 258ms/step - loss: 3.2001 - accuracy: 0.2434\n",
      "Epoch 76/166\n",
      "89/89 [==============================] - 25s 285ms/step - loss: 3.1754 - accuracy: 0.2527\n",
      "Epoch 77/166\n",
      "89/89 [==============================] - 22s 242ms/step - loss: 3.1540 - accuracy: 0.2530\n",
      "Epoch 78/166\n",
      "89/89 [==============================] - 20s 220ms/step - loss: 3.1274 - accuracy: 0.2581\n",
      "Epoch 79/166\n",
      "89/89 [==============================] - 18s 204ms/step - loss: 3.1044 - accuracy: 0.2636\n",
      "Epoch 80/166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 17s 195ms/step - loss: 3.0849 - accuracy: 0.2632\n",
      "Epoch 81/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 3.0621 - accuracy: 0.2717\n",
      "Epoch 82/166\n",
      "89/89 [==============================] - 18s 197ms/step - loss: 3.0317 - accuracy: 0.2759\n",
      "Epoch 83/166\n",
      "89/89 [==============================] - 18s 199ms/step - loss: 3.0086 - accuracy: 0.2813\n",
      "Epoch 84/166\n",
      "89/89 [==============================] - 16s 185ms/step - loss: 2.9949 - accuracy: 0.2838\n",
      "Epoch 85/166\n",
      "89/89 [==============================] - 16s 185ms/step - loss: 2.9657 - accuracy: 0.2917\n",
      "Epoch 86/166\n",
      "89/89 [==============================] - 16s 185ms/step - loss: 2.9435 - accuracy: 0.2946\n",
      "Epoch 87/166\n",
      "89/89 [==============================] - 16s 183ms/step - loss: 2.9226 - accuracy: 0.3046\n",
      "Epoch 88/166\n",
      "89/89 [==============================] - 16s 183ms/step - loss: 2.9008 - accuracy: 0.3026\n",
      "Epoch 89/166\n",
      "89/89 [==============================] - 16s 183ms/step - loss: 2.8740 - accuracy: 0.3113\n",
      "Epoch 90/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.8533 - accuracy: 0.3146\n",
      "Epoch 91/166\n",
      "89/89 [==============================] - 16s 185ms/step - loss: 2.8387 - accuracy: 0.3146\n",
      "Epoch 92/166\n",
      "89/89 [==============================] - 18s 199ms/step - loss: 2.8215 - accuracy: 0.3209\n",
      "Epoch 93/166\n",
      "89/89 [==============================] - 18s 197ms/step - loss: 2.7984 - accuracy: 0.3196\n",
      "Epoch 94/166\n",
      "89/89 [==============================] - 17s 189ms/step - loss: 2.7731 - accuracy: 0.3304\n",
      "Epoch 95/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.7563 - accuracy: 0.3331\n",
      "Epoch 96/166\n",
      "89/89 [==============================] - 17s 192ms/step - loss: 2.7406 - accuracy: 0.3373\n",
      "Epoch 97/166\n",
      "89/89 [==============================] - 17s 190ms/step - loss: 2.7261 - accuracy: 0.3374\n",
      "Epoch 98/166\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 2.7017 - accuracy: 0.3452\n",
      "Epoch 99/166\n",
      "89/89 [==============================] - 17s 189ms/step - loss: 2.6806 - accuracy: 0.3483\n",
      "Epoch 100/166\n",
      "89/89 [==============================] - 16s 185ms/step - loss: 2.6666 - accuracy: 0.3492\n",
      "Epoch 101/166\n",
      "89/89 [==============================] - 16s 184ms/step - loss: 2.6421 - accuracy: 0.3573\n",
      "Epoch 102/166\n",
      "89/89 [==============================] - 16s 182ms/step - loss: 2.6326 - accuracy: 0.3567\n",
      "Epoch 103/166\n",
      "89/89 [==============================] - 16s 185ms/step - loss: 2.6094 - accuracy: 0.3659\n",
      "Epoch 104/166\n",
      "89/89 [==============================] - 16s 182ms/step - loss: 2.5908 - accuracy: 0.3724\n",
      "Epoch 105/166\n",
      "89/89 [==============================] - 16s 181ms/step - loss: 2.5779 - accuracy: 0.3683\n",
      "Epoch 106/166\n",
      "89/89 [==============================] - 18s 200ms/step - loss: 2.5681 - accuracy: 0.3726\n",
      "Epoch 107/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.5381 - accuracy: 0.3805\n",
      "Epoch 108/166\n",
      "89/89 [==============================] - 16s 181ms/step - loss: 2.5283 - accuracy: 0.3822\n",
      "Epoch 109/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 2.5130 - accuracy: 0.3864\n",
      "Epoch 110/166\n",
      "89/89 [==============================] - 16s 185ms/step - loss: 2.4982 - accuracy: 0.3879\n",
      "Epoch 111/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.4734 - accuracy: 0.3937\n",
      "Epoch 112/166\n",
      "89/89 [==============================] - 17s 191ms/step - loss: 2.4828 - accuracy: 0.3908\n",
      "Epoch 113/166\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 2.4470 - accuracy: 0.4000\n",
      "Epoch 114/166\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 2.4299 - accuracy: 0.3974\n",
      "Epoch 115/166\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 2.4150 - accuracy: 0.4066\n",
      "Epoch 116/166\n",
      "89/89 [==============================] - 17s 191ms/step - loss: 2.3965 - accuracy: 0.4078\n",
      "Epoch 117/166\n",
      "89/89 [==============================] - 18s 205ms/step - loss: 2.3920 - accuracy: 0.4126\n",
      "Epoch 118/166\n",
      "89/89 [==============================] - 17s 189ms/step - loss: 2.3738 - accuracy: 0.4122\n",
      "Epoch 119/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.3520 - accuracy: 0.4143\n",
      "Epoch 120/166\n",
      "89/89 [==============================] - 17s 185ms/step - loss: 2.3516 - accuracy: 0.4174\n",
      "Epoch 121/166\n",
      "89/89 [==============================] - 17s 185ms/step - loss: 2.3274 - accuracy: 0.4224\n",
      "Epoch 122/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.3217 - accuracy: 0.4240\n",
      "Epoch 123/166\n",
      "89/89 [==============================] - 16s 185ms/step - loss: 2.3027 - accuracy: 0.4308\n",
      "Epoch 124/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.2827 - accuracy: 0.4315\n",
      "Epoch 125/166\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 2.2709 - accuracy: 0.4353\n",
      "Epoch 126/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.2676 - accuracy: 0.4341\n",
      "Epoch 127/166\n",
      "89/89 [==============================] - 17s 188ms/step - loss: 2.2332 - accuracy: 0.4458\n",
      "Epoch 128/166\n",
      "89/89 [==============================] - 17s 188ms/step - loss: 2.2254 - accuracy: 0.4443\n",
      "Epoch 129/166\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 2.2129 - accuracy: 0.4531\n",
      "Epoch 130/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.2097 - accuracy: 0.4433\n",
      "Epoch 131/166\n",
      "89/89 [==============================] - 17s 189ms/step - loss: 2.1888 - accuracy: 0.4558\n",
      "Epoch 132/166\n",
      "89/89 [==============================] - 17s 188ms/step - loss: 2.1735 - accuracy: 0.4569\n",
      "Epoch 133/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.1665 - accuracy: 0.4561\n",
      "Epoch 134/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.1494 - accuracy: 0.4586\n",
      "Epoch 135/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.1282 - accuracy: 0.4672\n",
      "Epoch 136/166\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 2.1159 - accuracy: 0.4683\n",
      "Epoch 137/166\n",
      "89/89 [==============================] - 16s 185ms/step - loss: 2.1056 - accuracy: 0.4726\n",
      "Epoch 138/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.0934 - accuracy: 0.4752\n",
      "Epoch 139/166\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 2.0839 - accuracy: 0.4770\n",
      "Epoch 140/166\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 2.0641 - accuracy: 0.4856\n",
      "Epoch 141/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 2.0496 - accuracy: 0.4836\n",
      "Epoch 142/166\n",
      "89/89 [==============================] - 18s 198ms/step - loss: 2.0413 - accuracy: 0.4851\n",
      "Epoch 143/166\n",
      "89/89 [==============================] - 17s 193ms/step - loss: 2.0276 - accuracy: 0.4899\n",
      "Epoch 144/166\n",
      "89/89 [==============================] - 17s 188ms/step - loss: 2.0169 - accuracy: 0.4921\n",
      "Epoch 145/166\n",
      "89/89 [==============================] - 17s 189ms/step - loss: 2.0032 - accuracy: 0.4929\n",
      "Epoch 146/166\n",
      "89/89 [==============================] - 17s 190ms/step - loss: 1.9935 - accuracy: 0.4974\n",
      "Epoch 147/166\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 1.9761 - accuracy: 0.5010\n",
      "Epoch 148/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 1.9686 - accuracy: 0.5002\n",
      "Epoch 149/166\n",
      "89/89 [==============================] - 17s 189ms/step - loss: 1.9546 - accuracy: 0.5027\n",
      "Epoch 150/166\n",
      "89/89 [==============================] - 17s 187ms/step - loss: 1.9378 - accuracy: 0.5081\n",
      "Epoch 151/166\n",
      "89/89 [==============================] - 17s 189ms/step - loss: 1.9339 - accuracy: 0.5073\n",
      "Epoch 152/166\n",
      "89/89 [==============================] - 20s 229ms/step - loss: 1.9171 - accuracy: 0.5104\n",
      "Epoch 153/166\n",
      "89/89 [==============================] - 19s 211ms/step - loss: 1.9077 - accuracy: 0.5117\n",
      "Epoch 154/166\n",
      "89/89 [==============================] - 19s 214ms/step - loss: 1.8971 - accuracy: 0.5165\n",
      "Epoch 155/166\n",
      "89/89 [==============================] - 17s 193ms/step - loss: 1.8794 - accuracy: 0.5230\n",
      "Epoch 156/166\n",
      "89/89 [==============================] - 17s 193ms/step - loss: 1.8749 - accuracy: 0.5184\n",
      "Epoch 157/166\n",
      "89/89 [==============================] - 17s 190ms/step - loss: 1.8597 - accuracy: 0.5252\n",
      "Epoch 158/166\n",
      "89/89 [==============================] - 17s 190ms/step - loss: 1.8360 - accuracy: 0.5306\n",
      "Epoch 159/166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 17s 187ms/step - loss: 1.8307 - accuracy: 0.5324\n",
      "Epoch 160/166\n",
      "89/89 [==============================] - 17s 186ms/step - loss: 1.8168 - accuracy: 0.5369\n",
      "Epoch 161/166\n",
      "89/89 [==============================] - 16s 184ms/step - loss: 1.8157 - accuracy: 0.5357\n",
      "Epoch 162/166\n",
      "89/89 [==============================] - 16s 183ms/step - loss: 1.7975 - accuracy: 0.5397\n",
      "Epoch 163/166\n",
      "89/89 [==============================] - 17s 195ms/step - loss: 1.7877 - accuracy: 0.5401\n",
      "Epoch 164/166\n",
      "89/89 [==============================] - 17s 194ms/step - loss: 1.7675 - accuracy: 0.5461\n",
      "Epoch 165/166\n",
      "89/89 [==============================] - 17s 188ms/step - loss: 1.7596 - accuracy: 0.5484\n",
      "Epoch 166/166\n",
      "89/89 [==============================] - 17s 188ms/step - loss: 1.7469 - accuracy: 0.5519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bb0c416848>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=166,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaaad1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
